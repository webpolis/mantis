% HMST: Hierarchical Memory-State Transformer
% Scientific Paper - NeurIPS 2024 Style

\documentclass{article}

% Packages
\usepackage[preprint]{neurips_2024}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

% Title and Authors
\title{HMST: A Proposed Hierarchical Memory-State Transformer Architecture for Hallucination Mitigation}

\author{%
  Anonymous Authors \\
  Institution Name \\
  \texttt{email@institution.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
Large Language Models (LLMs) exhibit a fundamental trade-off between long-context retention, computational efficiency, and hallucination rates. Existing approaches either scale quadratically with context length (dense transformers), compress information lossy (state-space models), or lack contextual statefulness (retrieval-augmented generation). We propose \textbf{HMST} (Hierarchical Memory-State Transformer), a novel hybrid architecture design that integrates a sparse Mixture-of-Experts (MoE) backbone with a meta-controlled three-tier memory system. We present a complete reference implementation where a lightweight \textit{Meta-Controller}, designed to be optimized via reinforcement learning (PPO), dynamically routes queries between: (1) immediate self-attention, (2) episodic state-space compression (Mamba-based SSM), and (3) semantic vector retrieval (FAISS). An optional critic model is included to provide hallucination detection through output verification. This paper details the architectural design and implementation of HMST as a framework for future research into adaptive memory systems. We discuss the theoretical properties of the architecture, its expected computational advantages, and the proposed training methodology required to validate its performance on long-context benchmarks.
\end{abstract>

\section{Introduction}

The success of Large Language Models \cite{brown2020gpt3,touvron2023llama,anthropic2024claude} has been accompanied by persistent challenges in long-context reasoning and factual accuracy. Current architectures face three fundamental limitations:

\textbf{(1) Quadratic Scaling:} Self-attention mechanisms in transformers scale as $O(L^2)$ with sequence length $L$, imposing prohibitive computational costs for contexts exceeding 8K tokens \cite{vaswani2017attention,dao2022flashattention}.

\textbf{(2) Lost-in-the-Middle:} Dense attention struggles to maintain uniform retrieval accuracy across extended contexts, exhibiting U-shaped performance curves where mid-sequence information is preferentially forgotten \cite{liu2023lost}.

\textbf{(3) Hallucination Persistence:} Despite advances in pre-training scale and alignment, LLMs continue to generate plausible but factually incorrect outputs, particularly when forced to extrapolate beyond training distributions \cite{zhang2023sirens,ji2023hallucination}.

Existing mitigation strategies exhibit complementary weaknesses. \textit{Retrieval-Augmented Generation} (RAG) \cite{lewis2020retrieval} offloads long-term memory to external datastores but lacks stateful compression of conversation context. \textit{State Space Models} (SSMs) \cite{gu2022s4,gu2023mamba} achieve linear complexity but sacrifice the ability to perform targeted retrieval over historical context. \textit{Sparse attention} patterns \cite{child2019sparse,beltagy2020longformer} reduce compute but maintain quadratic worst-case complexity.

\subsection{Contributions}

We propose HMST, a hierarchical architecture that integrates three memory tiers under unified RL-based control:

\begin{enumerate}
    \item \textbf{Sparse MoE Backbone:} A 12B-parameter base model design with 8 experts and top-2 routing, activating only 2B parameters per token for efficient processing.
    \item \textbf{Three-Tier Memory Hierarchy:}
    \begin{itemize}
        \item \textit{L1 (Working Memory):} Standard self-attention over immediate context ($\sim$2K tokens).
        \item \textit{L2 (Episodic Memory):} Input-dependent Mamba SSM compressing recent interactions into 256D states (8K effective tokens).
        \item \textit{L3 (Semantic Memory):} FAISS vector index enabling sub-linear retrieval over 1M+ historical entries.
    \end{itemize}
    \item \textbf{Meta-Controller Architecture:} A lightweight 6-layer transformer designed to be trained via PPO to dynamically route queries based on complexity, uncertainty, and compute budget.
    \item \textbf{Integrated Critic Model:} A 1B-parameter verification network designed to detect hallucinations through logical consistency checking.
\end{enumerate}

We provide a comprehensive description of the system architecture and its implementation, along with a theoretical analysis of its scaling properties and expected benefits over dense baselines.

\section{Related Work}

\paragraph{Sparse Models \& Mixture of Experts.} Switch Transformer \cite{fedus2022switch} and Mixtral \cite{jiang2024mixtral} demonstrate that sparse expert activation maintains model quality while reducing per-token compute. Our MoE backbone follows top-K routing principles but integrates with memory-augmented components rather than serving as the sole architecture.

\paragraph{State Space Models.} Structured SSMs (S4 \cite{gu2022s4}, Mamba \cite{gu2023mamba}, Jamba \cite{lieber2024jamba}) achieve linear complexity through selective state compression. While Jamba explores SSM-attention hybrids at the layer level, HMST implements hierarchical memory with explicit meta-control for routing decisions.

\paragraph{Memory-Augmented Networks.} Neural Turing Machines \cite{graves2014ntm} and Memory Networks \cite{weston2014memory} pioneered external memory interfaces. Modern RAG systems \cite{lewis2020retrieval,izacard2021atlas} combine dense retrievers with generative models. HMST extends this paradigm with multi-tier memory and learned access policies.

\paragraph{Hallucination Detection.} Recent work addresses factual errors through self-consistency checking \cite{manakul2023selfcheckgpt}, retrieval verification \cite{gao2023rarr}, and uncertainty quantification \cite{kuhn2023semantic}. Our critic model explicitly conditions on retrieved evidence for calibrated confidence estimation.

\section{Architecture}

\subsection{Notation and Problem Setup}

Given an input sequence $\mathbf{x} = (x_1, \ldots, x_L)$ where $x_i \in \mathcal{V}$ (vocabulary), we seek to model the conditional distribution $p(x_{t+1} | x_{\leq t})$ while maintaining three objectives:

\begin{align}
\text{Minimize:} \quad & \mathcal{L}_{LM} = -\sum_{t=1}^{L} \log p(x_t | x_{<t}) \quad \text{(Language Modeling)} \\
\text{Minimize:} \quad & \mathcal{C}_{compute}(x) \quad \text{(Computational Cost)} \\
\text{Maximize:} \quad & \mathcal{Q}_{factual}(x) \quad \text{(Factual Accuracy)}
\end{align}

Let $d_{model}$ denote embedding dimension, $N=8$ the number of experts, and $K=2$ the activation sparsity.

\subsection{Base Mixture-of-Experts Layer}

Each MoE layer consists of $N$ expert networks $\{E_i\}_{i=1}^N$ where $E_i: \mathbb{R}^{d_{model}} \to \mathbb{R}^{d_{model}}$ is a two-layer feed-forward network:
\begin{equation}
E_i(\mathbf{x}) = W_{2,i} \, \text{GELU}(W_{1,i} \mathbf{x} + \mathbf{b}_{1,i}) + \mathbf{b}_{2,i}
\end{equation}
where GELU is the Gaussian Error Linear Unit activation \cite{hendrycks2016gelu}.

The gating network $G(\mathbf{x})$ computes routing probabilities:
\begin{align}
\mathbf{h} &= W_g \mathbf{x} \in \mathbb{R}^N \\
\mathbf{p} &= \text{Softmax}(\mathbf{h})
\end{align}

Top-$K$ routing selects the expert subset $\mathcal{T} = \text{TopK}(\mathbf{p}, K=2)$ and renormalizes:
\begin{equation}
\mathbf{y} = \sum_{i \in \mathcal{T}} \frac{p_i}{\sum_{j \in \mathcal{T}} p_j} \, E_i(\mathbf{x})
\end{equation}

\textbf{Load Balancing.} To prevent expert collapse, we propose minimizing the auxiliary loss:
\begin{equation}
\mathcal{L}_{bal} = \alpha \cdot N \sum_{i=1}^N f_i \cdot \bar{p}_i
\end{equation}
where $f_i$ is the fraction of tokens routed to expert $i$ in the batch, $\bar{p}_i = \frac{1}{|\mathcal{B}|}\sum_{x \in \mathcal{B}} p_i(x)$ is the average routing probability across the batch, and $\alpha=0.01$ is a hyperparameter.

\subsection{Meta-Controller}

The Meta-Controller $\pi_\theta$ is a 6-layer transformer (150M parameters) that observes query embedding $\mathbf{e}_q$ and state summary $\mathbf{s}_t$:
\begin{equation}
\mathbf{s}_t = [\text{pos}_t, \text{uncert}_t, \text{len}_t, \text{conf}_t]
\end{equation}
where:
\begin{itemize}
    \item $\text{pos}_t$: Positional encoding
    \item $\text{uncert}_t$: Epistemic uncertainty from prior predictions
    \item $\text{len}_t$: Current context length (normalized)
    \item $\text{conf}_t$: Moving average of model confidence scores
\end{itemize}

The controller outputs five decision gates via separate linear heads:
\begin{align}
g_{exit} &= \sigma(W_{exit} \mathbf{z}_t) \quad \text{(Early Exit)} \\
g_{epi} &= \sigma(W_{epi} \mathbf{z}_t) \quad \text{(Episodic Access)} \\
g_{sem} &= \sigma(W_{sem} \mathbf{z}_t) \quad \text{(Semantic Access)} \\
g_{ver} &= \sigma(W_{ver} \mathbf{z}_t) \quad \text{(Verification)} \\
\boldsymbol{\pi}_{exp} &= \text{Softmax}(W_{exp} \mathbf{z}_t) \quad \text{(Expert Weights)}
\end{align}
where $\mathbf{z}_t = \text{TransformerEncoder}([\mathbf{e}_q; \mathbf{s}_t])$ and $\sigma$ is the sigmoid function.

Binary decisions are made via thresholding: $a_* = \mathbb{1}[g_* > \tau]$ with $\tau = 0.5$. The proposed training employs Gumbel-Softmax \cite{jang2017gumbel} for differentiability.

\subsection{Episodic Memory: Selective State Space Model}

The L2 episodic memory implements a selective SSM following the Mamba architecture \cite{gu2023mamba}. For input sequence $\{\mathbf{x}_t\}$, the continuous-time state-space equations are:
\begin{align}
\mathbf{h}'(t) &= \mathbf{A} \mathbf{h}(t) + \mathbf{B} \mathbf{x}(t) \\
\mathbf{y}(t) &= \mathbf{C} \mathbf{h}(t)
\end{align}

\textbf{Selective Discretization.} We compute input-dependent time-scales:
\begin{equation}
\Delta_t = \text{Softplus}(W_\Delta \mathbf{x}_t + \mathbf{b}_\Delta)
\end{equation}

The continuous parameters are discretized via zero-order hold:
\begin{align}
\bar{\mathbf{A}}_t &= \exp(\Delta_t \mathbf{A}) \\
\bar{\mathbf{B}}_t &= (\Delta_t \mathbf{A})^{-1}(\exp(\Delta_t \mathbf{A}) - \mathbf{I}) \Delta_t \mathbf{B} \label{eq:discretization}
\end{align}

The discrete recurrence becomes:
\begin{align}
\mathbf{h}_t &= \bar{\mathbf{A}}_t \mathbf{h}_{t-1} + \bar{\mathbf{B}}_t \mathbf{x}_t \\
\mathbf{y}_t &= \mathbf{C}_t \mathbf{h}_t + \mathbf{D} \mathbf{x}_t
\end{align}
where $\mathbf{B}_t = W_B \mathbf{x}_t$ and $\mathbf{C}_t = W_C \mathbf{x}_t$ provide input-dependent selectivity.

\textbf{Efficiency.} The recurrence processes 8K tokens into a fixed 256D state vector $\mathbf{h}_t$, enabling $O(1)$ memory footprint for retrieval.

\subsection{Semantic Memory: Vector Index Retrieval}

The L3 semantic memory uses a FAISS index \cite{johnson2019faiss} with IVF (Inverted File) and PQ (Product Quantization):
\begin{itemize}
    \item \textbf{Indexing:} Historical embeddings $\{\mathbf{v}_i\}_{i=1}^M$ are clustered into $n_{clusters}=\sqrt{M}$ Voronoi cells.
    \item \textbf{Quantization:} Each 1024D vector is compressed into 64 bytes via 8-byte sub-vectors.
\end{itemize}

For query $\mathbf{q}$, retrieval proceeds as:
\begin{equation}
\mathcal{R}_k(\mathbf{q}) = \underset{\mathbf{v} \in \mathcal{I}}{\text{TopK}} \left( \frac{\mathbf{q}^\top \mathbf{v}}{\|\mathbf{q}\| \|\mathbf{v}\|} \right)
\end{equation}
with sub-linear complexity $O(\sqrt{M})$ due to IVF clustering with $\approx\sqrt{M}$ clusters.

\textbf{Background Consolidation.} Importance scores $I(\mathbf{h}_t)$ are computed for episodic states:
\begin{equation}
I(\mathbf{h}_t) = \alpha \cdot \text{novelty}(\mathbf{h}_t) + (1-\alpha) \cdot \text{frequency}(\mathbf{h}_t)
\end{equation}
High-importance states are intended to be transferred from L2 to L3 during idle cycles.

\subsection{Critic Model for Hallucination Detection}

The critic $C_\phi$ is a 1B-parameter model that takes the query $\mathbf{q}$, generated response $\mathbf{r}$, and retrieved facts $\mathcal{F}$ as input:
\begin{equation}
P_{ver}, C_{ver} = C_\phi([\mathbf{q}; \mathbf{r}; \mathcal{F}])
\end{equation}
where $P_{ver} \in [0,1]$ is the correctness probability and $C_{ver} \in [0,1]$ is the confidence score.

Training minimizes the calibrated loss:
\begin{equation}
\mathcal{L}_{critic} = \text{BCE}(P_{ver}, y_{true}) + \lambda \|C_{ver} - y_{true}\|_2^2
\end{equation}
with $\lambda=0.5$ for balanced calibration.

\section{Proposed Training Methodology}

\subsection{Stage 1: Pre-training}

The base MoE model and memory modules are designed to be jointly trained on language modeling:
\begin{equation}
\mathcal{L}_{total} = \mathcal{L}_{LM} + \alpha \mathcal{L}_{bal}
\end{equation}

We propose using the AdamW optimizer \cite{loshchilov2019adamw} with learning rate $3 \times 10^{-4}$, batch size 512, and mixed-precision training (bf16). The target training corpus would consist of 2T tokens from diverse sources (web text, books, code).

\subsection{Stage 2: Meta-Controller Reinforcement Learning}

The Meta-Controller is designed to be optimized via Proximal Policy Optimization (PPO) \cite{schulman2017ppo} to maximize expected reward:
\begin{equation}
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \gamma^t R_t \right]
\end{equation}

\textbf{Reward Function:}
\begin{equation}
R_t = w_1 \mathbb{1}_{acc} - w_2 \frac{T_{lat}}{T_{base}} - w_3 \mathcal{C}_{compute} + w_4 \mathcal{Q}_{calib}
\end{equation}
where:
\begin{itemize}
    \item $\mathbb{1}_{acc}$: Accuracy (1 if correct, -1 otherwise)
    \item $T_{lat} / T_{base}$: Normalized latency (vs. full model inference)
    \item $\mathcal{C}_{compute} = \frac{\text{active\_params}}{\text{total\_params}}$: Compute cost
    \item $\mathcal{Q}_{calib} = -(u_t - (1 - y_{true}))^2$: Calibration quality
\end{itemize}

Weights: $w_1=1.0, w_2=0.3, w_3=0.2, w_4=0.5$.

\textbf{PPO Update:}
The policy is updated by minimizing the clipped surrogate loss:
\begin{equation}
\mathcal{L}^{PPO}(\theta) = -\mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right) \right]
\end{equation}
where $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ is the probability ratio, $\hat{A}_t$ is the Generalized Advantage Estimate \cite{schulman2016gae}, and $\epsilon=0.2$ is the clipping parameter.

\subsection{Stage 3: Critic Fine-Tuning}

The critic model is intended to be fine-tuned on curated hallucination datasets (HaluEval \cite{li2023halueval}, FaithDial \cite{dziri2022faithdial}) with:
\begin{itemize}
    \item Positive examples: Factually correct responses with supporting evidence
    \item Negative examples: Plausible but incorrect outputs with contradictory evidence
\end{itemize}

\section{Proposed Evaluation Protocol}

\subsection{Baselines}

We propose comparing HMST against the following baselines to validate its effectiveness:
\begin{itemize}
    \item \textbf{Llama-3-8B} \cite{touvron2023llama}: Dense transformer baseline
    \item \textbf{Mixtral-8x7B} \cite{jiang2024mixtral}: Sparse MoE without memory augmentation
    \item \textbf{Mamba-2.8B} \cite{gu2023mamba}: Pure SSM architecture
    \item \textbf{Llama-3-8B + RAG}: Dense model with FAISS retrieval
\end{itemize}

\subsection{Proposed Datasets}

\begin{itemize}
    \item \textbf{LongBench} \cite{bai2023longbench}: Long-context QA (16K-128K tokens)
    \item \textbf{TruthfulQA} \cite{lin2022truthfulqa}: Hallucination-prone questions
    \item \textbf{HaluEval} \cite{li2023halueval}: Synthetic hallucination detection
    \item \textbf{NarrativeQA} \cite{kocisky2018narrativeqa}: Book-length comprehension
\end{itemize}

\subsection{Proposed Metrics}

\begin{enumerate}
    \item \textbf{Accuracy:} Exact match / F1 score on QA tasks
    \item \textbf{Hallucination Rate:} Fraction of factually incorrect generations
    \item \textbf{Latency:} Wall-clock inference time (A100 GPU)
    \item \textbf{FLOPs:} Floating-point operations per token
    \item \textbf{Calibration Error:} Expected Calibration Error (ECE) \cite{guo2017calibration}
\end{enumerate}

\section{Expected Properties}

\subsection{Computational Efficiency}

The hierarchical memory design is expected to enable graceful degradation: simple queries should bypass expensive retrieval (Gate 1), while complex reasoning activates the full memory hierarchy. We anticipate that RL optimization will learn query-specific routing policies, potentially achieving significant speedups compared to dense baselines by activating the minimal necessary parameters for each token.

\subsection{Scaling Analysis}

Theoretically, HMST should maintain sub-quadratic complexity as context length increases, due to the $O(1)$ memory footprint of the episodic memory and $O(\sqrt{M})$ retrieval complexity of the semantic memory. This contrasts with dense baselines which exhibit quadratic growth.

\section{Discussion}

\subsection{Architectural Design Decisions}

The decision to separate memory into three tiers is motivated by the distinct access patterns required for effective language modeling. Working memory handles immediate syntax and local coherence; episodic memory maintains a compressed narrative thread; and semantic memory provides access to long-term facts. The meta-controller serves as a learned arbitrator, aiming to balance the trade-off between retrieval cost and information gain.

\subsection{Hallucination Mitigation Strategy}

The critic model is designed to improve calibration by conditioning verification on retrieved evidence. By explicitly modeling the confidence of generated outputs against retrieved facts, the system aims to filter out ungrounded generation. The expected failure modes would likely involve multi-hop reasoning where the necessary evidence is distributed across different memory tiers, requiring complex routing decisions.

\subsection{Limitations}

\begin{itemize}
    \item \textbf{Training Complexity:} RL optimization of the meta-controller presents a non-trivial stability challenge and requires careful hyperparameter tuning.
    \item \textbf{Memory Maintenance:} L3 index rebuilding scales as $O(M \log M)$ for $M$ entries, which may become a bottleneck for extremely large historical contexts.
    \item \textbf{Cold Start:} The system requires a warmup period to populate the episodic memory before it can effectively leverage historical context.
\end{itemize}

\section{Conclusion}

We have presented HMST, a proposed hierarchical memory-augmented transformer architecture designed to address fundamental trade-offs in long-context LLM design. By combining sparse MoE computation with multi-tier memory under RL-based meta-control, HMST offers a promising direction for achieving superior efficiency and factual accuracy. This paper details the complete architectural design and implementation, laying the groundwork for future training and evaluation. The proposed design suggests that learned routing policies could dynamically balance compute allocation based on query complexity.

Pending validation of the core architecture, future work could explore: (1) scaling memory capacity to 10M+ entries, (2) continual learning protocols for memory consolidation, and (3) integration with tool-augmented reasoning systems.

\section*{Broader Impact}

The proposed HMST architecture aims to improve reliability in high-stakes applications (medical QA, legal reasoning). However, the critic model might introduce brittleness if trained on biased datasets, potentially amplifying systemic biases. Any future deployment should include human-in-the-loop verification for critical decisions.

\begin{thebibliography}{99}

\bibitem{brown2020gpt3}
T. Brown et al., ``Language Models are Few-Shot Learners,'' \textit{NeurIPS}, 2020.

\bibitem{touvron2023llama}
H. Touvron et al., ``LLaMA: Open and Efficient Foundation Language Models,'' \textit{arXiv:2302.13971}, 2023.

\bibitem{anthropic2024claude}
Anthropic, ``Claude 3 Model Family,'' Technical Report, 2024.

\bibitem{vaswani2017attention}
A. Vaswani et al., ``Attention is All You Need,'' \textit{NeurIPS}, 2017.

\bibitem{dao2022flashattention}
T. Dao et al., ``FlashAttention: Fast and Memory-Efficient Exact Attention,'' \textit{NeurIPS}, 2022.

\bibitem{liu2023lost}
N. Liu et al., ``Lost in the Middle: How Language Models Use Long Contexts,'' \textit{TACL}, 2023.

\bibitem{zhang2023sirens}
Y. Zhang et al., ``Siren's Song in the AI Ocean,'' \textit{EMNLP}, 2023.

\bibitem{ji2023hallucination}
Z. Ji et al., ``Survey of Hallucination in Natural Language Generation,'' \textit{ACM Comp. Surveys}, 2023.

\bibitem{lewis2020retrieval}
P. Lewis et al., ``Retrieval-Augmented Generation for Knowledge-Intensive NLP,'' \textit{NeurIPS}, 2020.

\bibitem{gu2022s4}
A. Gu et al., ``Efficiently Modeling Long Sequences with Structured State Spaces,'' \textit{ICLR}, 2022.

\bibitem{gu2023mamba}
A. Gu and T. Dao, ``Mamba: Linear-Time Sequence Modeling with Selective State Spaces,'' \textit{arXiv:2312.00752}, 2023.

\bibitem{child2019sparse}
R. Child et al., ``Generating Long Sequences with Sparse Transformers,'' \textit{arXiv:1904.10509}, 2019.

\bibitem{beltagy2020longformer}
I. Beltagy et al., ``Longformer: The Long-Document Transformer,'' \textit{arXiv:2004.05150}, 2020.

\bibitem{fedus2022switch}
W. Fedus et al., ``Switch Transformers: Scaling to Trillion Parameter Models,'' \textit{JMLR}, 2022.

\bibitem{jiang2024mixtral}
A. Q. Jiang et al., ``Mixtral of Experts,'' \textit{arXiv:2401.04088}, 2024.

\bibitem{lieber2024jamba}
O. Lieber et al., ``Jamba: A Hybrid Transformer-Mamba Language Model,'' \textit{arXiv:2403.19887}, 2024.

\bibitem{graves2014ntm}
A. Graves et al., ``Neural Turing Machines,'' \textit{arXiv:1410.5401}, 2014.

\bibitem{weston2014memory}
J. Weston et al., ``Memory Networks,'' \textit{ICLR}, 2015.

\bibitem{izacard2021atlas}
G. Izacard et al., ``Atlas: Few-shot Learning with Retrieval Augmented Language Models,'' \textit{arXiv:2208.03299}, 2022.

\bibitem{manakul2023selfcheckgpt}
P. Manakul et al., ``SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection,'' \textit{EMNLP}, 2023.

\bibitem{gao2023rarr}
L. Gao et al., ``RARR: Researching and Revising What Language Models Say,'' \textit{ACL}, 2023.

\bibitem{kuhn2023semantic}
L. Kuhn et al., ``Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation,'' \textit{ICLR}, 2023.

\bibitem{hendrycks2016gelu}
D. Hendrycks and K. Gimpel, ``Gaussian Error Linear Units (GELUs),'' \textit{arXiv:1606.08415}, 2016.

\bibitem{jang2017gumbel}
E. Jang et al., ``Categorical Reparameterization with Gumbel-Softmax,'' \textit{ICLR}, 2017.

\bibitem{johnson2019faiss}
J. Johnson et al., ``Billion-scale Similarity Search with GPUs,'' \textit{IEEE Trans. Big Data}, 2019.

\bibitem{loshchilov2019adamw}
I. Loshchilov and F. Hutter, ``Decoupled Weight Decay Regularization,'' \textit{ICLR}, 2019.

\bibitem{schulman2017ppo}
J. Schulman et al., ``Proximal Policy Optimization Algorithms,'' \textit{arXiv:1707.06347}, 2017.

\bibitem{schulman2016gae}
J. Schulman et al., ``High-Dimensional Continuous Control Using Generalized Advantage Estimation,'' \textit{ICLR}, 2016.

\bibitem{li2023halueval}
Y. Li et al., ``HaluEval: A Large-Scale Hallucination Evaluation Benchmark,'' \textit{EMNLP}, 2023.

\bibitem{dziri2022faithdial}
N. Dziri et al., ``FaithDial: A Faithful Benchmark for Information-Seeking Dialogue,'' \textit{TACL}, 2022.

\bibitem{bai2023longbench}
Y. Bai et al., ``LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding,'' \textit{arXiv:2308.14508}, 2023.

\bibitem{lin2022truthfulqa}
S. Lin et al., ``TruthfulQA: Measuring How Models Mimic Human Falsehoods,'' \textit{ACL}, 2022.

\bibitem{kocisky2018narrativeqa}
T. Kocisk\'y et al., ``The NarrativeQA Reading Comprehension Challenge,'' \textit{TACL}, 2018.

\bibitem{guo2017calibration}
C. Guo et al., ``On Calibration of Modern Neural Networks,'' \textit{ICML}, 2017.

\end{thebibliography}

\end{document}
